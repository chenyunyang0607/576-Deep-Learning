# -*- coding: utf-8 -*-
"""tanh-tanh-momentum.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G-Mv2mvPDOBz3dnumeZgFsU-qBgCrGKr
"""

# -*- coding: utf-8 -*-
"""Assignment_1_Pytorch_MNIST.ipynb

Automatically generated by Colaboratory.

Overall structure:

1) Set Pytorch metada
- seed
- tensorflow output
- whether to transfer to gpu (cuda)

2) Import data
- download data
- create data loaders with batchsie, transforms, scaling

3) Define Model architecture, loss and optimizer

4) Define Test and Training loop
    - Train:
        a. get next batch
        b. forward pass through model
        c. calculate loss
        d. backward pass from loss (calculates the gradient for each parameter)
        e. optimizer: performs weight updates

5) Perform Training over multiple epochs:
    Each epoch:
    - call train loop
    - call test loop

Acknowledgments:https://github.com/motokimura/pytorch_tensorboard/blob/master/main.py
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms

from torch.utils.tensorboard import SummaryWriter
from torch.utils.data import DataLoader, random_split
from datetime import datetime
import os
from pathlib import Path


ACT_CONV = "tanh"
ACT_FC   = "tanh"
OPTIMIZER = "momentum"

batch_size = 64
test_batch_size = 1000
epochs = 10
lr = 0.01
try_cuda = True
seed = 1000
logging_interval = 100
logging_dir = None

# Defining the test and training loops
eps = 1e-13

cfg_tag = f"{ACT_CONV}-{ACT_FC}-{OPTIMIZER}"
run_tag = f"_c-{cfg_tag}"
logdir = (logging_dir or f"runs/mnist_{datetime.now().strftime('%Y%m%d-%H%M%S')}") + run_tag
writer = SummaryWriter(logdir)
print("logdir:", logdir)

if torch.cuda.is_available() and try_cuda:
    device = torch.device("cuda")
    torch.cuda.manual_seed(seed)
else:
    device = torch.device("cpu")
    torch.manual_seed(seed)
torch.backends.cudnn.benchmark = True
g_cpu = torch.Generator().manual_seed(seed)

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

full_train = datasets.MNIST("./data", train=True, download=True, transform=transform)
train_ds, val_ds = random_split(full_train, [55000, 5000], generator=g_cpu)

train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)
val_loader   = DataLoader(val_ds,   batch_size=test_batch_size, shuffle=False, num_workers=0)
test_loader  = DataLoader(datasets.MNIST("./data", train=False, download=True, transform=transform),
                          batch_size=test_batch_size, shuffle=False, num_workers=0)

def _act(x, kind):
    if kind == "relu": return F.relu(x, inplace=False)
    if kind == "leakyrelu": return F.leaky_relu(x, negative_slope=0.1)
    if kind == "tanh": return torch.tanh(x)
    if kind == "sigmoid": return torch.sigmoid(x)
    return x

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)
        self.conv2_drop = nn.Dropout(p=0.5)  # dropout after fc per forward
        self.fc1 = nn.Linear(64 * 7 * 7, 1024)
        self.fc2 = nn.Linear(1024, 10)
        self.cache = {}

    def forward(self, x):
        z1 = self.conv1(x)
        a1 = _act(z1, ACT_CONV)
        p1 = F.max_pool2d(a1, 2)

        z2 = self.conv2(p1)
        a2 = _act(z2, ACT_CONV)
        p2 = F.max_pool2d(a2, 2)

        f = torch.flatten(p2, 1)
        z3 = self.fc1(f)
        a3 = _act(z3, ACT_FC)
        a3 = self.conv2_drop(a3)  # dropout after fc
        z4 = self.fc2(a3)

        self.cache = {"z1": z1, "a1": a1, "p1": p1,
                      "z2": z2, "a2": a2, "p2": p2,
                      "z3": z3, "a3": a3, "z4": z4}
        return F.softmax(z4, dim=1)

model = Net().to(device)

def _init_weights(m):
    if isinstance(m, nn.Conv2d):
        nonlin = "relu" if ACT_CONV in ("relu", "leakyrelu") else "linear"
        nn.init.kaiming_normal_(m.weight, mode="fan_out",
                                nonlinearity="leaky_relu" if ACT_CONV=="leakyrelu" else nonlin)
        if m.bias is not None: nn.init.zeros_(m.bias)
    elif isinstance(m, nn.Linear):
        nn.init.xavier_normal_(m.weight)
        if m.bias is not None: nn.init.zeros_(m.bias)

model.apply(_init_weights)

# log graph once
_sample = torch.randn(1, 1, 28, 28, device=device)
try:
    writer.add_graph(model, _sample)
except Exception:
    pass

if OPTIMIZER == "adam":
    optimizer = optim.Adam(model.parameters(), lr=lr)
elif OPTIMIZER == "sgd":
    optimizer = optim.SGD(model.parameters(), lr=lr)
elif OPTIMIZER == "momentum":
    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)
else:
    optimizer = optim.Adam(model.parameters(), lr=lr)

def _init_stat_bucket():
    return {"step": [], "min": [], "max": [], "mean": [], "std": []}

STAT, LAST_SNAPSHOT = {}, {}
def _ensure(tag):
    if tag not in STAT: STAT[tag] = _init_stat_bucket()

def _update_stats(tag, tensor, step):
    t = tensor.detach().float().view(-1).cpu()
    _ensure(tag)
    STAT[tag]["step"].append(step)
    STAT[tag]["min"].append(float(t.min()))
    STAT[tag]["max"].append(float(t.max()))
    STAT[tag]["mean"].append(float(t.mean()))
    STAT[tag]["std"].append(float(t.std(unbiased=False)))
    LAST_SNAPSHOT[tag] = t.numpy()

train_steps, train_losses, train_accs = [], [], []
val_epochs,  val_losses,  val_accs  = [], [], []
test_epochs, test_losses, test_accs = [], [], []

def train(epoch):
    model.train()
    criterion = nn.NLLLoss()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(torch.log(output + eps), target)
        loss.backward()
        optimizer.step()

        if batch_idx % logging_interval == 0:
            pred = output.argmax(dim=1)
            acc = (pred == target).float().mean().item()
            n_iter = (epoch - 1) * len(train_loader) + batch_idx
            print(f"Train Epoch: {epoch} [{batch_idx*len(data)}/{len(train_loader.dataset)}] "
                  f"Loss: {loss.item():.4f}  Acc: {acc*100:.2f}%")
            writer.add_scalar("train/loss", loss.item(), n_iter)
            writer.add_scalar("train/acc",  acc,        n_iter)
            writer.add_scalar("train/error", 1.0 - acc, n_iter)

            train_steps.append(n_iter); train_losses.append(loss.item()); train_accs.append(acc)

            for name, param in model.named_parameters():
                tag = f"{cfg_tag}/param/{name}"
                _update_stats(tag, param, n_iter)
                writer.add_histogram(f"{tag}/hist", param.detach().cpu().numpy(), n_iter)
                writer.add_scalar(f"{tag}/min",  STAT[tag]["min"][-1],  n_iter)
                writer.add_scalar(f"{tag}/max",  STAT[tag]["max"][-1],  n_iter)
                writer.add_scalar(f"{tag}/mean", STAT[tag]["mean"][-1], n_iter)
                writer.add_scalar(f"{tag}/std",  STAT[tag]["std"][-1],  n_iter)
            for key, val in getattr(model, "cache", {}).items():
                tag = f"{cfg_tag}/act/{key}"
                _update_stats(tag, val, n_iter)
                writer.add_histogram(f"{tag}/hist", val.detach().cpu().numpy(), n_iter)
                writer.add_scalar(f"{tag}/min",  STAT[tag]["min"][-1],  n_iter)
                writer.add_scalar(f"{tag}/max",  STAT[tag]["max"][-1],  n_iter)
                writer.add_scalar(f"{tag}/mean", STAT[tag]["mean"][-1], n_iter)
                writer.add_scalar(f"{tag}/std",  STAT[tag]["std"][-1],  n_iter)

@torch.no_grad()
def _evaluate(loader, tag_prefix, epoch):
    model.eval()
    criterion = nn.NLLLoss(reduction="sum")
    total_loss, correct, total = 0.0, 0, 0
    for data, target in loader:
        data, target = data.to(device), target.to(device)
        output = model(data)
        total_loss += criterion(torch.log(output + eps), target).item()
        pred = output.argmax(dim=1)
        correct += pred.eq(target).sum().item()
        total += target.numel()
    avg_loss = total_loss / total
    acc = correct / total
    n_iter = epoch * len(train_loader)
    writer.add_scalar(f"{cfg_tag}/{tag_prefix}/loss",  avg_loss, n_iter)
    writer.add_scalar(f"{cfg_tag}/{tag_prefix}/acc",   acc,      n_iter)
    writer.add_scalar(f"{cfg_tag}/{tag_prefix}/error", 1.0 - acc, n_iter)
    return avg_loss, acc, correct, total

def validate(epoch):
    avg_loss, acc, correct, total = _evaluate(val_loader, "val", epoch)
    print(f"Validation: Average loss: {avg_loss:.4f}, Accuracy: {correct}/{total} ({100.0*acc:.2f}%)")
    val_epochs.append(epoch); val_losses.append(avg_loss); val_accs.append(acc)

def test(epoch):
    avg_loss, acc, correct, total = _evaluate(test_loader, "test", epoch)
    print(f"Test set: Average loss: {avg_loss:.4f}, Accuracy: {correct}/{total} ({100.0*acc:.2f}%)")
    test_epochs.append(epoch); test_losses.append(avg_loss); test_accs.append(acc)

for epoch in range(1, epochs + 1):
    train(epoch); validate(epoch); test(epoch)

writer.flush()
writer.close()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir {logdir} --reload_interval 20

